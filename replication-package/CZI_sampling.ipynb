{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af1b852",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import urllib.request\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "env_path = Path('.') / '.czi.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "# Configure the data dir path in .czi.env located in this directory\n",
    "DATA_DIR_PATH = os.environ.get(\"DATA_DIR_PATH\")\n",
    "print(f'Using data directory {DATA_DIR_PATH}')\n",
    "\n",
    "RAW_FILE_PATH = f'{DATA_DIR_PATH}/raw.tar.gz'\n",
    "LINKED_FILE_PATH = f'{DATA_DIR_PATH}/linked.tar.gz'\n",
    "RAW_TAR_URL = \"https://datadryad.org/stash/downloads/file_stream/1822384\"\n",
    "LINKED_TAR_URL = \"https://datadryad.org/stash/downloads/file_stream/1822388\"\n",
    "FILTERED_CZI_SOFTWARE_CSV = 'czi_software.csv'\n",
    "FULL_FILE_PATH = f'{DATA_DIR_PATH}/{FILTERED_CZI_SOFTWARE_CSV}'\n",
    "\n",
    "SAMPLED_100k_CZI_CSV = 'sample_100000.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1bdbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data directory and check if files exist to avoid duplicate large downloads / expensive computes\n",
    "raw_exists = False\n",
    "linked_exists = False\n",
    "full_exists = False\n",
    "try:\n",
    "    os.makedirs(DATA_DIR_PATH)\n",
    "except FileExistsError:\n",
    "    raw_exists = os.path.isfile(RAW_FILE_PATH)\n",
    "    linked_exists = os.path.isfile(LINKED_FILE_PATH)\n",
    "    full_exists = os.path.isfile(FULL_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cc256d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a progressbar to track download progress\n",
    "import progressbar\n",
    "\n",
    "class DownloadProgressBar():\n",
    "    def __init__(self):\n",
    "        self.pbar = None\n",
    "\n",
    "    def __call__(self, block_num, block_size, total_size):\n",
    "        if not self.pbar:\n",
    "            self.pbar=progressbar.ProgressBar(maxval=total_size)\n",
    "            self.pbar.start()\n",
    "\n",
    "        downloaded = block_num * block_size\n",
    "        if downloaded < total_size:\n",
    "            self.pbar.update(downloaded)\n",
    "        else:\n",
    "            self.pbar.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd50143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve data from CZI dataset into data directory\n",
    "import tarfile\n",
    "if not linked_exists:\n",
    "    urllib.request.urlretrieve(LINKED_TAR_URL, LINKED_FILE_PATH, DownloadProgressBar())\n",
    "    with tarfile.open(LINKED_FILE_PATH) as linked_tar:\n",
    "        linked_tar.extractall(f'{DATA_DIR_PATH}/linked/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a9f7da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not raw_exists:\n",
    "    urllib.request.urlretrieve(RAW_TAR_URL, RAW_FILE_PATH, DownloadProgressBar())\n",
    "    with tarfile.open(RAW_FILE_PATH) as raw_tar:\n",
    "        raw_tar.extractall(f'{DATA_DIR_PATH}/raw/')\n",
    "    # gunzip needed files\n",
    "    import gzip\n",
    "    import shutil\n",
    "    for gzip_file in [\n",
    "        f'{DATA_DIR_PATH}/raw/raw/comm_raw.tsv.gz',\n",
    "        f'{DATA_DIR_PATH}/raw/raw/non_comm_raw.tsv.gz',\n",
    "        f'{DATA_DIR_PATH}/raw/raw/publishers_collections_raw.tsv.gz',\n",
    "    ]:\n",
    "        with gzip.open(gzip_file, 'rb') as f_in:\n",
    "            with open(gzip_file[:-3], 'wb') as f_out:\n",
    "                shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500254ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dtypes for dataset\n",
    "\n",
    "_meta = {\n",
    "    'Unnamed: 0': 'object',\n",
    "    'license': 'str',\n",
    "    'location': 'str',\n",
    "    'pmcid': 'object',\n",
    "    'pmid': 'object',\n",
    "    'doi': 'object',\n",
    "    'pubdate': 'object',\n",
    "    'source': 'object',\n",
    "    'number': 'object',\n",
    "    'text': 'str',\n",
    "    'software': 'str',\n",
    "    'version': 'str',\n",
    "    'ID': 'str',\n",
    "    'curation_label': 'str',\n",
    "    'software_upper': 'str',\n",
    "    'mention_counts': 'int64',\n",
    "}\n",
    "\n",
    "if not full_exists:\n",
    "    #Take a look at the data and basic cleaning\n",
    "    # Use dask, as we're dealing with potentially larger-than-memory data (e.g. the raw publishers mentions dataset has 10+GB)\n",
    "    import dask.dataframe as dd\n",
    "\n",
    "    # Read TSV into a single dataframe, brute-force mapping all values to strings\n",
    "    df_czi = dd.concat([\n",
    "        dd.read_csv(f'{DATA_DIR_PATH}/raw/raw/comm_raw.tsv', sep='\\t', converters={i: str for i in range(14771000)}),\n",
    "        dd.read_csv(f'{DATA_DIR_PATH}/raw/raw/non_comm_raw.tsv', sep='\\t', converters={i: str for i in range(4547000)}),\n",
    "        dd.read_csv(f'{DATA_DIR_PATH}/raw/raw/publishers_collections_raw.tsv', sep='\\t', converters={i: str for i in range(48165000)})\n",
    "    ],\n",
    "    ignore_index=True)\n",
    "\n",
    "    # Filter only curated software\n",
    "    df_filtered_czi = df_czi.loc[df_czi['curation_label'] == 'software']\n",
    "    \n",
    "    # Add a new column with software names in uppercase for brute force deduplication\n",
    "    df_filtered_czi['software_upper'] = df_filtered_czi['software'].str.upper()\n",
    "    \n",
    "    # Add a column that contains value counts for software names\n",
    "    df_filtered_czi['mention_counts'] = df_filtered_czi['software_upper'].map(df_filtered_czi['software_upper'].value_counts())\n",
    "    \n",
    "    # Save the whole humungous dataframe to a single file\n",
    "    df_filtered_czi.to_csv(f\"{DATA_DIR_PATH}/{FILTERED_CZI_SOFTWARE_CSV}\", index=True, encoding='utf-8-sig', single_file=True)\n",
    "    full_exists = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3886a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "ddf_filtered_czi = dd.read_csv(f\"{DATA_DIR_PATH}/{FILTERED_CZI_SOFTWARE_CSV}\", encoding='utf-8-sig', dtype=_meta)\n",
    "\n",
    "software_series = ddf_filtered_czi.software\n",
    "distinct_software_counts = software_series.value_counts()\n",
    "\n",
    "#This is what our data looks like on a log scale\n",
    "plt.hist(distinct_software_counts, bins=1000)\n",
    "plt.yscale('log', nonpositive='clip')\n",
    "plt.savefig(f'{DATA_DIR_PATH}/czi_full_citation_histogram.png')\n",
    "with open(f'{DATA_DIR_PATH}/software_mentions_total', 'w') as tf:\n",
    "    tf.write(str(ddf_filtered_czi.index.size.compute()))\n",
    "\n",
    "with open(f'{DATA_DIR_PATH}/distinct_software_mentions_total', 'w') as tf:\n",
    "    tf.write(str(distinct_software_counts.size.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5eb798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a random stratified proportionate sample of 100,000 rows, and save as CSV\n",
    "SAMPLE_SIZE = 100000\n",
    "\n",
    "idx = ddf_filtered_czi.index.size.compute()\n",
    "\n",
    "# Take a stratified sample (on software name) of ~100k rows\n",
    "\n",
    "ddf_sample = ddf_filtered_czi.groupby('software_upper', group_keys=False).apply(\n",
    "    lambda x: x.sample(\n",
    "        int(\n",
    "            np.rint(SAMPLE_SIZE * len(x) / idx)\n",
    "        )), meta = _meta\n",
    ").sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Save sample\n",
    "\n",
    "ddf_sample.to_csv(f\"{DATA_DIR_PATH}/{SAMPLED_100k_CZI_CSV}\", index=True, encoding='utf-8-sig', single_file=True)\n",
    "\n",
    "software_sample_series = ddf_sample.software\n",
    "distinct_software_sample_counts = software_sample_series.value_counts()\n",
    "\n",
    "# Save the histogram and data count for comparison with the full data\n",
    "plt.hist(distinct_software_sample_counts,bins=1000)\n",
    "plt.yscale('log', nonpositive='clip')\n",
    "plt.savefig(f'{DATA_DIR_PATH}/czi_100000_sample_citation_histogram.png')\n",
    "with open(f'{DATA_DIR_PATH}/software_mentions_sample', 'w') as tf:\n",
    "    tf.write(str(ddf_sample.index.size.compute()))\n",
    "    \n",
    "with open(f'{DATA_DIR_PATH}/distinct_software_mentions_sample', 'w') as tf:\n",
    "    tf.write(str(distinct_software_sample_counts.size.compute()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c99134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Population is non-normal (see czi_100000_sample_citation_histogram.png) so to test variances use Levene's test,\n",
    "# Comparing the equality of variance between the full dataset and the sample\n",
    "# https://www.itl.nist.gov/div898/handbook/eda/section3/eda35a.htm\n",
    "\n",
    "from scipy.stats import levene\n",
    "\n",
    "stat,p=levene(distinct_software_counts, distinct_software_sample_counts)\n",
    "print(p)\n",
    "#Actual values of variances\n",
    "varval = [np.var(x, ddof=1) for x in [distinct_software_counts.compute(), distinct_software_sample_counts.compute()]]\n",
    "\n",
    "\n",
    "with open(f'{DATA_DIR_PATH}/sample_v_full_variance_p_values', 'w') as pf:\n",
    "    pf.write(f'p: {str(p)}\\n{str(varval)}')\n",
    "\n",
    "# If this gives small p-values, populations don't have equal variances (to be expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33428a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take one sample per software group\n",
    "\n",
    "ddf_distinct_software_sample = ddf_sample.groupby('software_upper').apply(lambda x: x.sample(1), meta=_meta).reset_index(drop=True)\n",
    "ddf_distinct_software_sample.to_csv(f'{DATA_DIR_PATH}/sample_distinct_software.csv', index=True, encoding='utf-8-sig', single_file=True)\n",
    "\n",
    "# Split the resulting dataframe into one with the top 100 most mentioned software\n",
    "# and one with a sample (n=1000) of the remainder\n",
    "\n",
    "df_top100 = ddf_distinct_software_sample.nlargest(100, 'mention_counts')\n",
    "df_top100.to_csv(f'{DATA_DIR_PATH}/sample_top_100_counts.csv', index=True, encoding='utf-8-sig', single_file=True)\n",
    "ddf_distinct_software_sample = dd.read_csv(f'{DATA_DIR_PATH}/sample_distinct_software.csv', dtype=_meta)\n",
    "df_without_top100 = ddf_distinct_software_sample.nsmallest(len(ddf_distinct_software_sample)-100, 'mention_counts')\n",
    "sample_1000 = df_without_top100.sample(frac=0.14564521).compute()  # Fraction for 1000 of 6866 (size of the distinct sample - top 100)\n",
    "sample_1000.to_csv(f'{DATA_DIR_PATH}/sample_1000_counts.csv', index=True, encoding='utf-8-sig')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
